{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "## Introduction\n",
    "\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Blackboard before the deadline (see Blackboard).\n",
    "    \n",
    "Learning goals:\n",
    "1. Brush up basics of vectors and matrices\n",
    "2. Get familiar with python\n",
    "3. Get familiar with activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exercises 1-6 you do not need to write any Python code, but you might find calculating the exercises with Python useful for practice. \n",
    "\n",
    "For exercises 7-10 (and for all future assignments), make sure that your plots are shown *in* the notebook when we open it; which you can achieve by saving the notebook when all plots are open and sending in this version. \n",
    "\n",
    "As mentioned in the course reader, in every assignment we will check whether your notebook code runs through without errors with the Cell->Run All command. You risk not receiving any points if this fails. We will not debug your code. Be sure to restart the notebook kernel from time to time (Kernel->Restart) and before submitting to notice it if you use old variable or function names that are still defined in the notebook kernel, but not anymore in the code. \n",
    "\n",
    "A useful reference for linear algebra is [**The Matrix Cookbook**](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274). A good overview for partial derivatives is [Khan Academy: Partial Derivatives](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives-and-the-gradient/a/introduction-to-partial-derivatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1: Vector operations (1 point)\n",
    "Let's look at vectors. Work out this assignment by hand and write down your solution in markdown with LaTeX.\n",
    "\n",
    "Let $\\mathbf{x} = (1,2)^T$ and $\\mathbf{y} = (-1,1)^T$\n",
    "\n",
    "1. How much is $10\\mathbf{x}$?\n",
    "1. What is the length (norm) of the vector $\\mathbf{x}$? Briefly show how to calculate the solution. \n",
    "1. How much is $\\mathbf{x}^T\\mathbf{y}$?\n",
    "1. What is the angle between $\\mathbf{x}$ and $\\mathbf{y}$ in degrees? Briefly show how to calculate the solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "$$x = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} $$\n",
    "$$y = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$$\n",
    "1. $$10x = 10 * \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ 20 \\end{pmatrix}$$\n",
    "2. $$||x|| = \\sqrt{1^2 + 2^2} = \\sqrt{5}$$\n",
    "3. $$\\begin{pmatrix} 1 & 2 \\end{pmatrix}\\cdot\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = -1 + 2 = 1$$\n",
    "4. $$||x|| = \\sqrt{1^2 + 2^2} = \\sqrt{5}$$\n",
    "$$||y|| = \\sqrt{(-1)^2 + 1^2} = \\sqrt{2}$$\n",
    "$$Angle = cos(\\theta) = \\frac{x\\cdot y}{||x|| * ||y||} = \\frac{1}{\\sqrt{10}}$$\n",
    "$$\\theta = arccos(\\frac{1}{\\sqrt{10}}) = 71.57^\\circ \\approx 72^\\circ$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Vectors and matrices (1 point)\n",
    "Let's look at vectors and matrices. Work out this assignment by hand and write down your solution in markdown using LaTeX. \n",
    "\n",
    "Let $\\mathbf{x} = (1,2)^T$ and $\\mathbf{A} = \n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{array}\n",
    "\\right)\n",
    "$.\n",
    "\n",
    "1. Can we compute $\\mathbf{x}\\mathbf{A}$? Why can, or why can't we?\n",
    "1. How much is $\\mathbf{A}\\mathbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2\n",
    "1. We can't because the dimensions don't match. In order to multiply two matrices their dimensions have to be of the form \"axb bxc\" while those of $\\mathbf{xA}$ are of the form \"2x1 2x2\"\n",
    "2. $$\\mathbf{Ax} = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 11 \\end{pmatrix}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Matrices (1 point)\n",
    "Let's look at matrices. Work out this assignment by hand and write down your solution in markdown with LaTeX equations. \n",
    "\n",
    "Let $\\mathbf{A} = \n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{array}\n",
    "\\right)\n",
    "$ and $\\mathbf{B} = \n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{array}\n",
    "\\right)\n",
    "$.\n",
    "\n",
    "1. How much is $AB$?\n",
    "1. How much is $BA$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3\n",
    "1. $$\\begin{pmatrix} 19 & 22 \\\\ 43 & 50 \\end{pmatrix}$$\n",
    "2. $$\\begin{pmatrix} 23 & 34 \\\\ 31 & 46 \\end{pmatrix}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Partial derivatives (1 point)\n",
    "\n",
    "Let's brush up on partial derivatives. \n",
    "\n",
    "Let $\\mathbf{x} = (x_1,\\ldots,x_i,\\ldots,x_n)^T$ (a vector) and $f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{x}$. Write down the expression for the partial derivative $\\frac{\\partial f}{\\partial x_i}$. Briefly explain how you arrived at the result. \n",
    "\n",
    "Hint: How would the function $f(\\mathbf{x})$ look like if it was written with the vector scalars $x_i$ instead of the vector $\\mathbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4\n",
    "$$f(x) = \\sum_{i=1}^{n} x_i^2$$\n",
    "$$\\frac{\\partial f}{\\partial x_i} = \\frac{\\partial\\sum_{i=1}^{n} x_i^2}{\\partial x_i} = 2x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Gradients (1 point)\n",
    "Often, we need to compute the gradient of a particular function. Given a function $f(x_1,\\ldots,x_n)$, the gradient is just a collection of partial derivatives:\n",
    "\\begin{equation*}\n",
    "\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\ldots,\\frac{\\partial f}{\\partial x_n}\\right) \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "Let $f(x,y) = - (\\cos^2 x + \\cos^2 y)^2$. \n",
    "\n",
    "Derive the gradient $\\nabla f = \\left(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5\n",
    "$$\\frac{\\partial f}{\\partial x} = -2(cos^2(x) + cos^2(y)) * -2sin(x) = -2(cos^2(x) + cos^2(y)) * 2cos(x) * -sin(x) = -2(cos^2(x) + cos^2(y)) * - sin(2x) = -2(cos^2(x) + cos^2(y)) * -sin(2x)$$\n",
    "$$\\frac{\\partial f}{\\partial y} = -2(cos^2(x) + cos^2(y)) * -2sin(y) = -2(cos^2(x) + cos^2(y)) * 2cos(y) * -sin(y) = -2(cos^2(x) + cos^2(y)) * - sin(2y) = -2(cos^2(x) + cos^2(y)) * -sin(2y)$$\n",
    "So\n",
    "$$\\nabla f = \\left(-2(cos^2(x) + cos^2(y)) * -sin(2x), -2(cos^2(x) + cos^2(y)) * -sin(2y)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Linear activation function (1 point)\n",
    "Write a function that computes the *linear activation function* (trivial identity) for any given input, and plot it over the input range  $x \\in [-10,10]$. Don't forget to add sensible labels to the axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15d8e8a1320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear activation function: \n",
    "\n",
    "def lin_act_fun(x):\n",
    "    return x\n",
    "# Plot activation over given range: \n",
    "x = range(-10,11)\n",
    "y = lin_act_fun(x)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"input\")\n",
    "plt.ylabel(\"output\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Linear threshold activation function (1 point)\n",
    "Write a function that computes the *linear threshold activation function* (also known as step activation function) for any given input, and plot it over the input range $x \\in [-10,10]$. Don't forget to add sensible labels to the axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear threshold activation function: \n",
    "\n",
    "\n",
    "# Plot activation over given range: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Sigmoid activation function (1 point)\n",
    "Write a function that computes the *sigmoid activation function* for any given input, and plot it over the input range $x \\in [-10,10]$. Don't forget to add sensible labels to the axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear sigmoid activation function: \n",
    "\n",
    "\n",
    "# Plot activation over given range: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Inner products (1 point)\n",
    "\n",
    "**1.** The input of the activation function in a simple perceptron (or any regular neural network neuron) is calculated as a weighted sum between each input value $x_i$ and each corresponding weight $w_i$, that is: $\\sum_{i=1}^m w_i x_i $.\n",
    "\n",
    "Calculate the input of the activation function for the given input values ```x_inputs``` and weight values ```weights``` **in a for-loop**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solution 9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_inputs = np.array([4.0,2.0,3.0])\n",
    "weights  = np.array([0.7,0.3,0.2])\n",
    "\n",
    "print \"Shape of inputs: \", x_inputs.shape\n",
    "print \"Shape of weights: \", weights.shape\n",
    "\n",
    "activation_input = 0.0\n",
    "\n",
    "# Write a for-loop\n",
    "\n",
    "print \"The input of the activation function is:\", activation_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** For-loops tend to be slow. There is a direct mathematical operation that expresses the same as our weighted sum above. This operation is also efficiently implemented as a ```numpy``` function. \n",
    "\n",
    "How is the operation called? Use the corresponding ```numpy``` function to calculate ```activation_input``` in one line without a for-loop.\n",
    "\n",
    "Hint: $\\sum_{i=1}^m w_i x_i = \\mathbf{w}^\\top \\mathbf{x}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_inputs = np.array([4.0,2.0,3.0])\n",
    "weights  = np.array([0.7,0.3,0.2])\n",
    "\n",
    "print \"Shape of inputs: \", x_inputs.shape\n",
    "print \"Shape of weights: \", weights.shape\n",
    "\n",
    "# Write a one-liner\n",
    "\n",
    "print \"The input of the activation function is:\", activation_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: Inner products (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** When implementing a full neural network we will have multiple $h_n$ hidden units (think $h_n$ individual perceptrons). In a multi-layer perceptron (a simple fully connected neural network), every hidden unit $h_i$ is connected to all of the $m$ input units, leading to $m \\times h_n$ weights in total. Again, first implement this in a for-loop. In the example below `weights` represents the weights for 4 hidden units. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_inputs = np.array([4.0,2.0,3.0])\n",
    "weights  = np.array([[0.7,0.3,0.2], \n",
    "                     [-0.23,0.42,-0.1], \n",
    "                     [-1.5,-2.3,0.4], \n",
    "                     [0.83,-0.12,-0.7]])\n",
    "\n",
    "print \"Shape of inputs: \", x_inputs.shape\n",
    "print \"Shape of weights: \", weights.shape\n",
    "\n",
    "activation_inputs = np.zeros([weights.shape[0],])\n",
    "\n",
    "# Write a for-loop\n",
    "\n",
    "print \"The inputs of the activation functions for the hidden units are:\", activation_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Now implement the same with the operation you found before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_inputs = np.array([4.0,2.0,3.0])\n",
    "weights  = np.array([[0.7,0.3,0.2], \n",
    "                     [-0.23,0.42,-0.1], \n",
    "                     [-1.5,-2.3,0.4], \n",
    "                     [0.83,-0.12,-0.7]])\n",
    "\n",
    "print \"Shape of inputs: \", x_inputs.shape\n",
    "print \"Shape of weights: \", weights.shape\n",
    "\n",
    "activation_inputs = np.zeros([weights.shape[0],])\n",
    "\n",
    "# Write a one-liner\n",
    "\n",
    "print \"The inputs of the activation functions for the hidden units are:\", activation_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Usually you would process multiple examples at once (*in a batch*), generating a unit activation individually for every example. `x_inputs` now carries two examples. Now - using only the one-line operation you found before - again gather the activations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_inputs = np.array([[4.0,2.0,3.0], \n",
    "                     [3.0,0.5,4.0]])\n",
    "\n",
    "weights  = np.array([[0.7,0.3,0.2], \n",
    "                     [-0.23,0.42,-0.1], \n",
    "                     [-1.5,-2.3,0.4], \n",
    "                     [0.83,-0.12,-0.7]])\n",
    "\n",
    "print \"Shape of inputs: \", x_inputs.shape\n",
    "print \"Shape of weights: \", weights.shape\n",
    "\n",
    "activation_inputs = np.zeros([weights.shape[0], 2])\n",
    "\n",
    "# Write a one-liner\n",
    "\n",
    "print \"The 2 sets of inputs of the activation functions for the hidden units are:\"\n",
    "print activation_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
